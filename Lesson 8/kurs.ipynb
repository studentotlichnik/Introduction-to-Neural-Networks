{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "metodich5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDwCMNk-iJJK",
        "colab_type": "text"
      },
      "source": [
        "# Практическое задание к курсу"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAaxApu7iMqc",
        "colab_type": "text"
      },
      "source": [
        "**1. Обучите нейронную сеть любой архитектуры которой не было на курсе, либо обучите нейронную сеть разобранной архитектуры, но на том датасете, которого не было на уроках. Сделайте анализ, того, что вам помогло в улучшения работы нейронной сети.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILq7izOayuO_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "2879d481-91cd-4a66-e05c-d3e82258c554"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6SJGqYVhlU1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff7957a5-5a74-4a80-ce50-005ac872a665"
      },
      "source": [
        "import copy, numpy as np\n",
        "np.random.seed(0)\n",
        "from __future__ import print_function\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Activation\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "from keras.layers.recurrent import SimpleRNN, LSTM, GRU"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQpKF3CaDddN",
        "colab_type": "text"
      },
      "source": [
        "Исходный вариант из методички"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brRZx_aNDN3e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "83a51c61-97f4-4fbe-9b3f-652f8b554952"
      },
      "source": [
        "# построчное чтение из примера с текстом \n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/Deep mind's Go.txt\", 'rb') as _in:\n",
        "    lines = []\n",
        "    for line in _in:\n",
        "        line = line.strip().lower().decode(\"ascii\", \"ignore\")\n",
        "        if len(line) == 0:\n",
        "            continue\n",
        "        lines.append(line)\n",
        "text = \" \".join(lines)\n",
        "chars = set([c for c in text])\n",
        "nb_chars = len(chars)\n",
        "\n",
        "\n",
        "# создание индекса символов и reverse mapping чтобы передвигаться между значениями numerical\n",
        "# ID and a specific character. The numerical ID will correspond to a column\n",
        "# ID и определенный символ. Numerical ID будет соответсвовать колонке\n",
        "# число при использовании one-hot кодировки для представление входов символов\n",
        "char2index = {c: i for i, c in enumerate(chars)}\n",
        "index2char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "# для удобства выберете фиксированную длину последовательность 10 символов \n",
        "SEQLEN, STEP = 10, 1\n",
        "input_chars, label_chars = [], []\n",
        "\n",
        "# конвертация data в серии разных SEQLEN-length субпоследовательностей\n",
        "for i in range(0, len(text) - SEQLEN, STEP):\n",
        "    input_chars.append(text[i: i + SEQLEN])\n",
        "    label_chars.append(text[i + SEQLEN])\n",
        "\n",
        "\n",
        "# Вычисление one-hot encoding входных последовательностей X и следующего символа (the label) y\n",
        "\n",
        "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
        "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
        "for i, input_char in enumerate(input_chars):\n",
        "    for j, ch in enumerate(input_char):\n",
        "        X[i, j, char2index[ch]] = 1\n",
        "    y[i, char2index[label_chars[i]]] = 1\n",
        "\n",
        "\n",
        "# установка ряда метапамертров  для нейронной сети и процесса тренировки\n",
        "BATCH_SIZE, HIDDEN_SIZE = 128, 128\n",
        "NUM_ITERATIONS = 1 # 25 должно быть достаточно\n",
        "NUM_EPOCHS_PER_ITERATION = 1\n",
        "NUM_PREDS_PER_EPOCH = 100\n",
        "\n",
        "\n",
        "# Create a super simple recurrent neural network. There is one recurrent\n",
        "# layer that produces an embedding of size HIDDEN_SIZE from the one-hot\n",
        "# encoded input layer. This is followed by a Dense fully-connected layer\n",
        "# across the set of possible next characters, which is converted to a\n",
        "# probability score via a standard softmax activation with a multi-class\n",
        "# cross-entropy loss function linking the prediction to the one-hot\n",
        "# encoding character label.\n",
        "\n",
        "'''\n",
        "Создание очень простой рекуррентной нейронной сети. В ней будет один реккурентный закодированный входной слой. За ним последует полносвязный слой связанный с набором возможных следующих символов, которые конвертированы в вероятностные результаты через стандартную softmax активацию с multi-class cross-encoding loss функцию ссылающуются на предсказание one-hot encoding лейбл символа\n",
        "'''\n",
        "\n",
        "model = Sequential()\n",
        "model.add(\n",
        "    GRU(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n",
        "        HIDDEN_SIZE,\n",
        "        return_sequences=False,\n",
        "        input_shape=(SEQLEN, nb_chars),\n",
        "        unroll=True\n",
        "    )\n",
        ")\n",
        "model.add(Dense(nb_chars))\n",
        "model.add(Activation(\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
        "\n",
        "\n",
        "# выполнение серий тренировочных и демонстрационных итераций \n",
        "for iteration in range(NUM_ITERATIONS):\n",
        "\n",
        "    # для каждой итерации запуск передачи данных в модель \n",
        "    print(\"=\" * 50)\n",
        "    print(\"Итерация #: %d\" % (iteration))\n",
        "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
        "\n",
        "    # Select a random example input sequence.\n",
        "    test_idx = np.random.randint(len(input_chars))\n",
        "    test_chars = input_chars[test_idx]\n",
        "\n",
        "    # для числа шагов предсказаний использование текущей тренируемой модели \n",
        "    # конструирование one-hot encoding для тестирования input и добавление предсказания.\n",
        "    print(\"Генерация из посева: %s\" % (test_chars))\n",
        "    print(test_chars, end=\"\")\n",
        "    for i in range(NUM_PREDS_PER_EPOCH):\n",
        "\n",
        "        # здесь one-hot encoding.\n",
        "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
        "        for j, ch in enumerate(test_chars):\n",
        "            X_test[0, j, char2index[ch]] = 1\n",
        "\n",
        "        # осуществление предсказания с помощью текущей модели.\n",
        "        pred = model.predict(X_test, verbose=0)[0]\n",
        "        y_pred = index2char[np.argmax(pred)]\n",
        "\n",
        "        # вывод предсказания добавленного к тестовому примеру \n",
        "        print(y_pred, end=\"\")\n",
        "\n",
        "        # инкрементация тестового примера содержащего предсказание\n",
        "        test_chars = test_chars[1:] + y_pred\n",
        "print()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Итерация #: 0\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 22s 300us/step - loss: 2.7884\n",
            "Генерация из посева: 15. all of\n",
            "15. all of antion antion antion antion antion antion antion antion antion antion antion antion antion antion a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgEZI4yGDm4n",
        "colab_type": "text"
      },
      "source": [
        "Увеличим NUM_ITERATIONS с 1 до 25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_WVkhxVDN8L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "92187103-38a5-4f1e-c4c3-a2ea6bd41b87"
      },
      "source": [
        "# построчное чтение из примера с текстом \n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/Deep mind's Go.txt\", 'rb') as _in:\n",
        "    lines = []\n",
        "    for line in _in:\n",
        "        line = line.strip().lower().decode(\"ascii\", \"ignore\")\n",
        "        if len(line) == 0:\n",
        "            continue\n",
        "        lines.append(line)\n",
        "text = \" \".join(lines)\n",
        "chars = set([c for c in text])\n",
        "nb_chars = len(chars)\n",
        "\n",
        "\n",
        "# создание индекса символов и reverse mapping чтобы передвигаться между значениями numerical\n",
        "# ID and a specific character. The numerical ID will correspond to a column\n",
        "# ID и определенный символ. Numerical ID будет соответсвовать колонке\n",
        "# число при использовании one-hot кодировки для представление входов символов\n",
        "char2index = {c: i for i, c in enumerate(chars)}\n",
        "index2char = {i: c for i, c in enumerate(chars)}\n",
        "\n",
        "# для удобства выберете фиксированную длину последовательность 10 символов \n",
        "SEQLEN, STEP = 10, 1\n",
        "input_chars, label_chars = [], []\n",
        "\n",
        "# конвертация data в серии разных SEQLEN-length субпоследовательностей\n",
        "for i in range(0, len(text) - SEQLEN, STEP):\n",
        "    input_chars.append(text[i: i + SEQLEN])\n",
        "    label_chars.append(text[i + SEQLEN])\n",
        "\n",
        "\n",
        "# Вычисление one-hot encoding входных последовательностей X и следующего символа (the label) y\n",
        "\n",
        "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
        "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
        "for i, input_char in enumerate(input_chars):\n",
        "    for j, ch in enumerate(input_char):\n",
        "        X[i, j, char2index[ch]] = 1\n",
        "    y[i, char2index[label_chars[i]]] = 1\n",
        "\n",
        "\n",
        "# установка ряда метапамертров  для нейронной сети и процесса тренировки\n",
        "BATCH_SIZE, HIDDEN_SIZE = 128, 128\n",
        "NUM_ITERATIONS = 25\n",
        "NUM_EPOCHS_PER_ITERATION = 1\n",
        "NUM_PREDS_PER_EPOCH = 100\n",
        "\n",
        "\n",
        "# Create a super simple recurrent neural network. There is one recurrent\n",
        "# layer that produces an embedding of size HIDDEN_SIZE from the one-hot\n",
        "# encoded input layer. This is followed by a Dense fully-connected layer\n",
        "# across the set of possible next characters, which is converted to a\n",
        "# probability score via a standard softmax activation with a multi-class\n",
        "# cross-entropy loss function linking the prediction to the one-hot\n",
        "# encoding character label.\n",
        "\n",
        "'''\n",
        "Создание очень простой рекуррентной нейронной сети. В ней будет один реккурентный закодированный входной слой. За ним последует полносвязный слой связанный с набором возможных следующих символов, которые конвертированы в вероятностные результаты через стандартную softmax активацию с multi-class cross-encoding loss функцию ссылающуются на предсказание one-hot encoding лейбл символа\n",
        "'''\n",
        "\n",
        "model = Sequential()\n",
        "model.add(\n",
        "    GRU(  # вы можете изменить эту часть на LSTM или SimpleRNN, чтобы попробовать альтернативы\n",
        "        HIDDEN_SIZE,\n",
        "        return_sequences=False,\n",
        "        input_shape=(SEQLEN, nb_chars),\n",
        "        unroll=True\n",
        "    )\n",
        ")\n",
        "model.add(Dense(nb_chars))\n",
        "model.add(Activation(\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
        "\n",
        "\n",
        "# выполнение серий тренировочных и демонстрационных итераций \n",
        "for iteration in range(NUM_ITERATIONS):\n",
        "\n",
        "    # для каждой итерации запуск передачи данных в модель \n",
        "    print(\"=\" * 50)\n",
        "    print(\"Итерация #: %d\" % (iteration))\n",
        "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
        "\n",
        "    # Select a random example input sequence.\n",
        "    test_idx = np.random.randint(len(input_chars))\n",
        "    test_chars = input_chars[test_idx]\n",
        "\n",
        "    # для числа шагов предсказаний использование текущей тренируемой модели \n",
        "    # конструирование one-hot encoding для тестирования input и добавление предсказания.\n",
        "    print(\"Генерация из посева: %s\" % (test_chars))\n",
        "    print(test_chars, end=\"\")\n",
        "    for i in range(NUM_PREDS_PER_EPOCH):\n",
        "\n",
        "        # здесь one-hot encoding.\n",
        "        X_test = np.zeros((1, SEQLEN, nb_chars))\n",
        "        for j, ch in enumerate(test_chars):\n",
        "            X_test[0, j, char2index[ch]] = 1\n",
        "\n",
        "        # осуществление предсказания с помощью текущей модели.\n",
        "        pred = model.predict(X_test, verbose=0)[0]\n",
        "        y_pred = index2char[np.argmax(pred)]\n",
        "\n",
        "        # вывод предсказания добавленного к тестовому примеру \n",
        "        print(y_pred, end=\"\")\n",
        "\n",
        "        # инкрементация тестового примера содержащего предсказание\n",
        "        test_chars = test_chars[1:] + y_pred\n",
        "print()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Итерация #: 0\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 22s 298us/step - loss: 2.7767\n",
            "Генерация из посева: th higher \n",
            "th higher the the the the the the the the the the the the the the the the the the the the the the the the the ==================================================\n",
            "Итерация #: 1\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 21s 289us/step - loss: 2.2628\n",
            "Генерация из посева: ee extende\n",
            "ee extended the search to the search to the search to the search to the search to the search to the search to ==================================================\n",
            "Итерация #: 2\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 21s 289us/step - loss: 2.0408\n",
            "Генерация из посева: algorithms\n",
            "algorithms the search programs and the search programs and the search programs and the search programs and the==================================================\n",
            "Итерация #: 3\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 21s 290us/step - loss: 1.8885\n",
            "Генерация из посева: nts. using\n",
            "nts. using the search the search the search the search the search the search the search the search the search ==================================================\n",
            "Итерация #: 4\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 21s 289us/step - loss: 1.7678\n",
            "Генерация из посева: e learning\n",
            "e learning the search and the state search and the state search and the state search and the state search and ==================================================\n",
            "Итерация #: 5\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 21s 289us/step - loss: 1.6710\n",
            "Генерация из посева: ames again\n",
            "ames against the policy network program and the programs program and the programs program and the programs pro==================================================\n",
            "Итерация #: 6\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 21s 290us/step - loss: 1.5882\n",
            "Генерация из посева: istics in \n",
            "istics in the policy network programs and the policy network programs and the policy network programs and the ==================================================\n",
            "Итерация #: 7\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 22s 291us/step - loss: 1.5171\n",
            "Генерация из посева:  elo ratin\n",
            " elo rating and and the policy network p? state search and policy network p? state search and policy network p==================================================\n",
            "Итерация #: 8\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 21s 290us/step - loss: 1.4566\n",
            "Генерация из посева: search art\n",
            "search article reseatch article reseatch article reseatch article reseatch article reseatch article reseatch a==================================================\n",
            "Итерация #: 9\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 21s 291us/step - loss: 1.4014\n",
            "Генерация из посева: f computat\n",
            "f computation of the search tree alphago wins by resignation of the search tree alphago wins by resignation of==================================================\n",
            "Итерация #: 10\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 22s 293us/step - loss: 1.3513\n",
            "Генерация из посева: if the act\n",
            "if the actions with a probability distributed alphago wins by research article research article research artic==================================================\n",
            "Итерация #: 11\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 22s 299us/step - loss: 1.3057\n",
            "Генерация из посева:  the value\n",
            " the value network p? and the search tree search tree search tree search tree search tree search tree search t==================================================\n",
            "Итерация #: 12\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 22s 294us/step - loss: 1.2637\n",
            "Генерация из посева: 54 55 56 5\n",
            "54 55 56 57 59 50 61 62 63 64 65 66 67 69 60 61 62 63 64 65 66 67 69 60 61 62 63 64 65 66 67 69 60 61 62 63 64==================================================\n",
            "Итерация #: 13\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 21s 290us/step - loss: 1.2269\n",
            "Генерация из посева: , inria (2\n",
            ", inria (2012). 52. computer games we the ras policy pradien than in precessing the respontin training and fro==================================================\n",
            "Итерация #: 14\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 21s 290us/step - loss: 1.1906\n",
            "Генерация из посева: 89] 84 [74\n",
            "89] 84 [74; 86] 8 [0; 19] 40 [16; 65] - 55 [55; 65] - 65 [55; 66] 8 [0; 19] 40 [16; 65] - 55 [55; 65] - 65 [55==================================================\n",
            "Итерация #: 15\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 21s 290us/step - loss: 1.1568\n",
            "Генерация из посева: 38 asynchr\n",
            "38 asynchronous 1 28 293 20 22 23 23 34 35 56 57 58 59 50 61 63 63 65 65 66 67 68 69 70 71 72 73 74 75 76 77 7==================================================\n",
            "Итерация #: 16\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 21s 288us/step - loss: 1.1255\n",
            "Генерация из посева: increasing\n",
            "increasing the station of the station of the station of the station of the station of the station of the stati==================================================\n",
            "Итерация #: 17\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 21s 290us/step - loss: 1.0956\n",
            "Генерация из посева: n which ko\n",
            "n which kosi in a simulations and programs are arvilicy pracks and a single machine learning of position s? po==================================================\n",
            "Итерация #: 18\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 22s 292us/step - loss: 1.0671\n",
            "Генерация из посева: ons of com\n",
            "ons of computer go programs and the search tree approximately and controbsted in an action value network and t==================================================\n",
            "Итерация #: 19\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 22s 292us/step - loss: 1.0406\n",
            "Генерация из посева: 3d). at ea\n",
            "3d). at each of the policy network p? provide a procable 10 100 102 103 104 105 106 107 108 109 100 101 102 10==================================================\n",
            "Итерация #: 20\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 22s 293us/step - loss: 1.0147\n",
            "Генерация из посева:  maximum o\n",
            " maximum of 5 search the ent of the strongest of the strongest of the strongest of the strongest of the strong==================================================\n",
            "Итерация #: 21\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 22s 291us/step - loss: 0.9907\n",
            "Генерация из посева:  ) ( )= ( \n",
            " ) ( )= ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ) ( ==================================================\n",
            "Итерация #: 22\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 22s 293us/step - loss: 0.9677\n",
            "Генерация из посева:  is played\n",
            " is played by the policy network p? and the outcome of games of games of games of games of games of games of g==================================================\n",
            "Итерация #: 23\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 22s 294us/step - loss: 0.9445\n",
            "Генерация из посева: valuating \n",
            "valuating the policy network p? and the policy network p? and the policy network p? and the policy network p? ==================================================\n",
            "Итерация #: 24\n",
            "Epoch 1/1\n",
            "73895/73895 [==============================] - 22s 296us/step - loss: 0.9225\n",
            "Генерация из посева: f planes d\n",
            "f planes dethors were allont games, r. & milery search the extended data table 1 | detains in the game of go. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFRi3s12D6fW",
        "colab_type": "text"
      },
      "source": [
        "**Выводы:** \n",
        "\n",
        "1. сгенерирован текст planes dethors were allont games, r. & milery search the extended data table 1 | detains in the game of go. \n",
        "\n",
        "2. loss существенно уменьшился по сравнению с первой (одной) итерацией. Текст сгенерирован более или менее осмысленный и по теме."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4xrZbMcIE7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu9G9xFiii_8",
        "colab_type": "text"
      },
      "source": [
        "**2. Сделайте краткий обзор какой-нибудь научной работы посвященной тому или иному алгоритму нейронных сетей, который не рассматривался на курсе. Проведите анализ: Чем отличается выбранная вами на рассмотрение архитектура нейронной сети от других архитектур? В чем плюсы и минусы данной архитектуры? Какие могут возникнуть трудности при применении данной архитектуры на практике?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh3RWfr3ijCm",
        "colab_type": "text"
      },
      "source": [
        "Адрес научной статьи: https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_Qo47gWijFJ",
        "colab_type": "text"
      },
      "source": [
        "AlphaGo - это программно-аппаратный комплекс для игры в Го.\n",
        "\n",
        "Го - это древняя игра. Ее родиной считается Китай, либо Япония (оспаривается).\n",
        "\n",
        "AlphaGo - разработана Google DeepMind в 2015 году. AlphaGo была первой в мире программой, которая выиграла профессионального Го-игрока без гандикапа (без форы) на стандартной доске 19 × 19 клеток.\n",
        "\n",
        "Эта победа стала важным прорывом в области искусственного интеллекта, поскольку большинство экспертов по искусственному интеллекту полагали, что такая программа не будет создана до 2020–2025 гг.\n",
        "Все предыдущие попытки создать программу аналогичной силы с использованием классического программирования заканчивались неудачей."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w75HYto4ijH-",
        "colab_type": "text"
      },
      "source": [
        "Существенной особенностью AlphaGo является то, что она использует общие алгоритмы, которые практически не зависят от особенностей игры. Алгоритм AlphaGo содержит только основные принципы игры, с которых любой начинающий игрок начинает изучать игру. Например, подсчет количества свободных очков из группы камней и анализ способности захватывать камни с помощью «лестницы». Остальное AlphaGo изучил самостоятельно, проанализировав базу данных из 160 тысяч игр, используя общие методы, которые можно использовать в других областях искусственного интеллекта."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlrg7d80ijMv",
        "colab_type": "text"
      },
      "source": [
        "AlphaGo работает с использованием нейронных сетей, которые успешно используются в распознавании образов. Крупный прорыв в этой области был связан с использованием многоуровневых сверточных нейронных сетей и специальной методики их обучения - глубокого обучения. Сверточные нейронные сети состоят из нескольких уровней нейронов. Каждый уровень получает матрицу чисел на входе, объединяет их с некоторыми весами и, используя нелинейную функцию активации, выдает много чисел на выходе, которые передаются на следующий уровень. При распознавании изображений изображение подается на первый уровень, а последний уровень дает результат. Нейронные сети тренируются на большом количестве изображений, постоянно корректируя веса, используемые для расчета результата. В результате нейронная сеть имеет конфигурацию, которая способна распознавать похожие изображения. Этот процесс не может быть предсказан, поэтому трудно сказать, как «мыслит» нейронная сеть, но, грубо говоря, выходные данные на промежуточных уровнях соответствуют различным возможным классификациям."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC2a1XTEi0TE",
        "colab_type": "text"
      },
      "source": [
        "Представление игровой позиции\n",
        "\n",
        "AlphaGo использует свёрточные нейронные сети для того, чтобы оценить позицию или предсказать следующий ход. Аналогично тому, как при распознавании образов на нейронную сеть подаётся изображение, AlphaGo подаёт на нейронные сети позицию. Каждая позиция представлена как многослойная картинка 19x19, где каждый слой представляет описания простых свойств каждого пункта доски. Используются следующие простые свойства: цвет камня, количество свободных пунктов у данной группы камней (если их не больше 8), взятие камней, возможность сделать ход в данный пункт, был ли данный камень поставлен недавно и т.д. Единственное нетривиальное свойство, которое используется, — это угрожает ли данной группе захват в лестницу. Всего используется 48 бинарных свойств (свойства, выраженные целым числом, представляются при помощи унитарного кода). Таким образом каждая позиция представлена в виде таблицы бит 19x19x48."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nazYCN7Yi0Vk",
        "colab_type": "text"
      },
      "source": [
        "Стратегическая сеть\n",
        "\n",
        "Чтобы не учитывать очень плохие ходы и, тем самым, уменьшить степень ветвления при поиске, AlphaGo использует стратегические сети (policy networks) - нейронные сети, которые помогают выбрать хороший ход.\n",
        "\n",
        "Одна из этих сетей (SL policy networks) может предсказать курс, который профессионал сделает в этой позиции. Эта 13-уровневая нейронная сеть получается путем обучения «с учителем» (supervised learning, SL) на 30 миллионах позиций, взятых из 160 тысяч игр, сыгранных на сервере KGS игроками с 6 по 9 дан. Обучение проходило в течение четырех недель на 50 графических процессорах.\n",
        "\n",
        "В качестве алгоритма обучения для поиска максимальной вероятности использовался стохастический градиентный спуск. Полученная нейронная сеть рассчитала распределение вероятностей среди всех возможных ходов в данной позиции (представленных, как описано выше). В результате нейронная сеть была в состоянии правильно предсказать курс, выбранный человеком в 57% тестовых ситуаций (не использованных в обучении). Для сравнения, лучший результат до AlphaGo составил 44%. Даже небольшое увеличение точности предсказания значительно увеличивает силу игры.\n",
        "\n",
        "Стратегическая сеть способна играть самостоятельно, выбирая каждый раз случайный ход с вычисленной вероятностью."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AAjVZeBi0YF",
        "colab_type": "text"
      },
      "source": [
        "Улучшенная стратегическая сеть\n",
        "\n",
        "Стратегическая сеть была улучшена с помощью обучения с подкреплением (reinforcement learning, RL), а именно: сеть постоянно улучшалась, играя с одной из сетей, полученных ранее. Более того, каждый раз случайная сеть выбиралась из ранее полученных, чтобы избежать переобучения (ситуации, когда программа выбирает лучший ход, предполагая, что противник использует ту же стратегию, но может плохо сыграть против другого противника). Результатом стала стратегическая сеть (RL policy network), которая превзошла исходную сеть в 80% игр.\n",
        "\n",
        "Оказалось, что получившаяся стратегическая сеть, без использования функции оценки или перечисления опций, смогла выиграть 85% игр против самой мощной в то время программы Pachi с открытым исходным кодом. Для сравнения, до этого лучшая программа, в которую играли без перечисления опций, а только с помощью сверточной нейронной сети, выиграла 11% игр у Pachi. Таким образом, AlphaGo, не перечисляя опций, смог достичь примерно любителя третьего уровня, указанного авторами программы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_YYOmhvi0ah",
        "colab_type": "text"
      },
      "source": [
        "Быстрая стратегическая сеть\n",
        "\n",
        "Чтобы смоделировать игру методом Монте-Карло, AlphaGo использует более быструю, но менее точную версию стратегической сети (rollout policy), которая получает ответ всего за 2 мкс. Эта быстрая сеть предсказывает ход человека с вероятностью 30%, в то время как улучшенная стратегическая сеть дает ответ на том же оборудовании в течение 3 мс с вероятностью 57%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24HAY_Wsi0dC",
        "colab_type": "text"
      },
      "source": [
        "Оценочная сеть\n",
        "\n",
        "Чтобы уменьшить глубину поиска, AlphaGo использовала оценочную сеть. Эта нейронная сеть оценивает вероятность выигрыша в данной позиции. Эта сеть является результатом обучения на 30 миллионах позиций, полученных в результате игры сама с собой в улучшенной стратегической сети. В этом случае в каждой игре было выбрано не более одной позиции (чтобы избежать переобучения из-за сходства позиций в одной игре). Для каждой из этих позиций была оценена вероятность выигрыша по методу Монте-Карло: был организован турнир из множества партий, в котором улучшенная стратегическая сеть, построенная на последнем этапе, играла сама с собой, начиная с этой позиции. После этого сеть оценки прошла обучение по этим данным. Обучение заняло одну неделю на 50 графических процессорах. Результатом стала сеть, которая могла предсказать вероятность выигрыша для каждой позиции, используя при этом в 15 000 раз меньше вычислений, чем метод Монте-Карло."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqFLOU6ni0fr",
        "colab_type": "text"
      },
      "source": [
        "Дерево поиска решения\n",
        "\n",
        "AlphaGo перебирает варианты, используя метод Монте-Карло для поиска в дереве, таким образом, что строит частичное игровое дерево из текущей позиции, производя многочисленные игровые симуляции. Для каждого хода в дереве записывается оценка, которая особым образом зависит от оценок ходов, полученных с использованием стратегических и оценочных сетей, от результата случайных игр в предыдущих симуляциях и от количества предыдущих симуляций, которые выбирали этот ход (чем чаще этот ход, тем ниже оценка, чтобы программа учитывала более разнообразные ходы).\n",
        "\n",
        "В начале каждой симуляции AlphaGo выбирает ход в уже построенном дереве с максимальным счетом. Когда симуляция достигает позиции, которой нет в дереве, эта позиция добавляется в дерево вместе со всеми ходами, разрешенными в этой позиции, которые оцениваются с использованием стратегической сети. Далее, как и в методе Монте-Карло, игра моделируется до конца без ветвления. В этом моделировании каждый ход выбирается случайным образом с вероятностью, полученной с помощью быстрой стратегической сети.\n",
        "\n",
        "В конце моделирования, в зависимости от результата, оценки ходов в построенном дереве обновляются. Таким образом, каждая симуляция начинается с текущей игровой позиции, доходит до конца, и в результате одной симуляции раскрывается одна позиция в текущем дереве.\n",
        "\n",
        "Авторы программы обнаружили, что на данном этапе выгоднее использовать не улучшенную стратегическую сеть, а начальную (SL policy network). По мнению авторов, это связано с тем, что профессиональные игроки выбирают более разнообразные ходы, чем улучшенная сеть, что позволяет программе рассмотреть больше вариантов. Таким образом, улучшенная стратегическая сеть не используется во время игры, но ее использование необходимо для построения оценочной сети, когда программа учится, играя сама с собой."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOWWkTiDi0iG",
        "colab_type": "text"
      },
      "source": [
        "Аппаратные средства\n",
        "\n",
        "Обучение AlphaGo проходило в течение нескольких недель на 50 графических процессорах с использованием платформы Google Cloud.\n",
        "\n",
        "AlphaGo был протестирован в компьютерных системах с разным количеством процессоров и графических процессоров, работающих параллельно или распределенными. В каждом случае, на ход было дано 2 секунды.\n",
        "\n",
        "Версия, выигравшая Fan Hui в октябре 2015 года, работала на 1202 процессорах и 176 графических процессорах.\n",
        "\n",
        "В игре с Ли Седолем в марте 2016 года AlphaGo использовала 1920 процессоров и 280 графических процессоров, работающих в распределенной сети."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm9geu4wijLC",
        "colab_type": "text"
      },
      "source": [
        "**Анализ:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLd1GQUOjSb7",
        "colab_type": "text"
      },
      "source": [
        "**Чем отличается выбранная вами на рассмотрение архитектура нейронной сети от других архитектур?**\n",
        "\n",
        "Особенностью выбранной архитектуры является то, что в AlphaGo нейросети используются как для оценки позиции (это, фактически, распознавание образа на \"картинке\" - игровой доске 19х19 с черными и белыми камнями), так и для управления перебором.\n",
        "Сочетание этих двух факторов дает такую чудовищную силу игры."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79e0cXXqjSep",
        "colab_type": "text"
      },
      "source": [
        "**В чем плюсы и минусы данной архитектуры?**\n",
        "\n",
        "Плюсы:\n",
        "- огромная сила игры - лучшая в мире;\n",
        "- архитектура создана на базу хорошо изученных сверточных нейросетей.\n",
        "\n",
        "Минусы:\n",
        "- требовательность к ресурсам. Обычный пользователь (любитель Го) не имеет доступа к таким ресурсам."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXPHkqOVjSjt",
        "colab_type": "text"
      },
      "source": [
        "**Какие могут возникнуть трудности при применении данной архитектуры на практике?**\n",
        "\n",
        "- высокое энергопотребление;\n",
        "- необходимость в очень мощных TPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll7xbXQ4jRmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "metodich2.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGh5mqEIxq0U",
        "colab_type": "text"
      },
      "source": [
        "# Введение в искусственные нейронные сети\n",
        "# Урок 3. Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5464Y0hxq0W",
        "colab_type": "text"
      },
      "source": [
        "## Содержание методического пособия:\n",
        "\n",
        "\n",
        "<ol>\n",
        "<li>Способы создания нейросетей</li>\n",
        "<li>Что такое Keras</li>\n",
        "<li>Основы синтаксиса</li>\n",
        "<li>Простая нейросеть на Keras</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY5SEd5Exq0X",
        "colab_type": "text"
      },
      "source": [
        "## Способы создания нейросетей\n",
        "\n",
        "Нейросети это математические модели. Программирую на любом языке можно решать задачи связанные с математикой. Однако встает вопрос какой язык подойдет для этого больше? Не считая учебных нейросетей, нейросети как правило работают с большим количеством данных. Поэтому, чтобы обучение нейросетей происходило с приемлимой скоростью нужно использовать быстрый язык. Например Си. Но так как язык Си это язык с низким уровнем абстракции то программировать и модифицировать на нем нейросети было бы крайне затруднительно. \n",
        "\n",
        "Хорошо может подойти для этих целей язык Python. Так как он с одной стороны имеет высокий уровень абстракции с другой стороны операции с массивами данных могут сделать его библиотеки написанные на Си. Таким способом мы пользовались на первых 2 уроках. Однако если писать нейросети таким образом то будет много повторяющегося кода поскольку архитектуры нейросетей остаются одинаковыми и зачастую у них только меняются параметры. Кроме этого нам может понадобиться хорошо знать архитектуры самых разных нейронных сетей чтобы реализовать их вручную. Работа таким образом затруднительна для людей не имеющих достаточной подготовки, а для имеющих может быть нааборот рутиной.\n",
        "\n",
        "Существуют фреймворки для созданий нейронных сетей. Они являются, пожалуй основным рабочим способом создания нейронных сетей. Вот их неполный перечень:\n",
        "\n",
        "1. TensorFlow\n",
        "2. PyTorch\n",
        "3. Keras\n",
        "4. Microsoft Cognitive Toolkit (CNTK)\n",
        "5. Caffe\n",
        "6. Apache MXNet\n",
        "\n",
        "Упрощение создания нейронных сетей не заканчивается на этих фрейворках. Существуют инструменты которые позволяют создавать нейронные сети без навыков программирования, строя нейросети графически. Примеры: Neural Designer, Deep Learning Studio.\n",
        "\n",
        "Но и на этом не заканчиваются способы создания нейросетей. Существуют инструменты самостоятельно создающие нейронные сети. Это так называемые AutoML инструменты. Вот примеры популярных из них:\n",
        "1. MLBox\n",
        "2. TPOT\n",
        "3. Autokeras\n",
        "\n",
        "Как вы возможно заметили что все эти инструменты отранжированы походы изложения в порядке возрастания уровня абстракции. Соответсвенно говоря о плюсах минусах того или иного инструмента мы должны понимать в принципе плюсы минусы повышения уровня абстракции. Чем он выше тем меньше производительность и тем меньше его гибкость и набоорот.\n",
        "\n",
        "Как уже было сказано наиболее востребованных в рабочих целях является тот уровень абстракции, который дают фреймворки. Будем изучать дальше и пользовать ими. Остается сделать выбор среди них. Самый популярный фреймворк для создания нейросетей TensorFlow. Самый популярный для обучения - Keras. На этом уроке мы изучим с вами Keras, а на следующим TensorFlow. Также стоит отметить, что эти фреймворки взаимосвязаны - Keras как правило работает поверх TensorFlow, а сам TensorFlow позволяет пользовать средствами Keras при необходимости.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSvqr3YSxq0X",
        "colab_type": "text"
      },
      "source": [
        "## Что такое Keras\n",
        "\n",
        "Keras появился относительно недавно - в 2015 г. Но за это время стал одним из самых популярных фреймоворков для создания нейросетей и фактически стандартом для использования его начинающими.\n",
        "\n",
        "В чем причина его популярности? Keras позволяет создовать на высоком уровне абстракции. Т.е. на не нужно вручную реализовать с помощью математикаподобного кода те или иные элементы нейронной сети. Мы можем оперировать слоями, количеством нейронов в них, выбором функции активации и т.д. В тоже время keras содержит инструментарий для всего того, что может понадобиться для работы - например ряд встроенных датасетов, возможность обрабатывать изображения.\n",
        "\n",
        "В техническом плане Keras это оболочка над инструментами меньшей степени абстракции. На выбор он может работать поверх TensorFlow, Microsoft Cognitive Toolkit, R, Theano, PlaidML.\n",
        "\n",
        "Keras пользуется также на соревнованиях Kaggle.\n",
        "\n",
        "Однако стоит отметить, что в реальных проектах чаще используется TensorFlow, который мы будем изучать в след. уроках.\n",
        "\n",
        "Keras как и любой высокобастрактный инструмент имеет изъяны в качестве меньшей гибкостью и производительснотью чем тот же tensorflow.\n",
        "\n",
        "Стоит также отметить, что Google официально поддерживает Keras, его автор François Chollet, является сотрудником Google. TensorFlow сам в свою очередь позволяет использовать возможности Keras, т.е. в нем заложена возможность переходить на более высокой уровень абстракции.\n",
        "\n",
        "В данном уроке мы с вами рассмотрим пример обучения нейронной сети с помощью Keras. Но прежде давайте посмотрим на основы синтаксиса Keras и стандартные задачи, которые нужно выполнить при обучении нейронной сети."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6ipbiAnxq0Y",
        "colab_type": "text"
      },
      "source": [
        "## Основы синтаксиса"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-HT5Yslxq0Z",
        "colab_type": "text"
      },
      "source": [
        "**Установка и работа с данными**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEVqarFLxq0Z",
        "colab_type": "text"
      },
      "source": [
        "Для начала необходимо установить keras. Надо полагать вы хорошо знакомы с командой pip."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB46N4Sxxq0a",
        "colab_type": "text"
      },
      "source": [
        "sudo python3 pip install keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1WQWpicxq0b",
        "colab_type": "text"
      },
      "source": [
        "Давайте попробуем получить датасет mnist и проанализировать его содержимое.\n",
        "Это еще не будет синтаксис Keras, но это часто встречающаяся задача. Не обращайте внимание на предупреждения от TensorFlow. Их часто бывает много и их можно подавить при необходимости."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jxX9fAvxq0b",
        "colab_type": "text"
      },
      "source": [
        "    import numpy as np\n",
        "    import mnist\n",
        "    import keras\n",
        "\n",
        "    # The first time you run this might be a bit slow, since the\n",
        "    # mnist package has to download and cache the data.\n",
        "    train_images = mnist.train_images()\n",
        "    train_labels = mnist.train_labels()\n",
        "\n",
        "    print(train_images.shape) # (60000, 28, 28)\n",
        "    print(train_labels.shape) # (60000,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJx1PLfixq0c",
        "colab_type": "text"
      },
      "source": [
        "Что в данном случае мы смогли с вами узнать? Что тренировочный датасет mnist состоит из 60000 изображений 28 на 28 пикселей. Такие небольшие датасеты с маленькими изображениями встретятся вам и в других учебных датасетах."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybLOSmd8xq0d",
        "colab_type": "text"
      },
      "source": [
        "Что нам нужно делать теперь? Если скаченный нами датасет не имеет разделения на тренировочный и тестовый то поделить их. В нашем случае наш тренировочный датасет состоит из 60 000 изображений и тестовый из 10 000 и они поделены по умолчанию.\n",
        "\n",
        "Нам теперь нужно конверитировать значения пикселей из вида от 1 до 255 в набор значений от -0.5 до 0.5.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NImtkgnxt8x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "66e7cb37-bf59-442a-a30c-2f41dd3f152f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTjIbO_dxq0d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "5373a6b2-4aab-4f16-be1a-f72a36709bc2"
      },
      "source": [
        "# !pip install keras"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8oOmdkcxq0j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "16cfaff9-d68e-45e6-ff95-a45b2afaf9b4"
      },
      "source": [
        "# !pip install mnist"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mnist\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/c4/5db3bfe009f8d71f1d532bbadbd0ec203764bba3a469e4703a889db8e5e0/mnist-0.2.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mnist) (1.18.4)\n",
            "Installing collected packages: mnist\n",
            "Successfully installed mnist-0.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hki-HHoxq0m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "feed7fb5-33ad-4581-d9ed-bf0525bb548a"
      },
      "source": [
        " # !pip install tensorflow"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.29.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow) (47.1.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0.post3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX5URs6Gxq0p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a3f595d0-b1c2-4b8b-d69c-7eaa629ac0c1"
      },
      "source": [
        "import numpy as np\n",
        "import mnist\n",
        "\n",
        "train_images = mnist.train_images()\n",
        "train_labels = mnist.train_labels()\n",
        "test_images = mnist.test_images()\n",
        "test_labels = mnist.test_labels()\n",
        "\n",
        "# Normalize the images.\n",
        "train_images = (train_images / 255) - 0.5\n",
        "test_images = (test_images / 255) - 0.5\n",
        "\n",
        "# Flatten the images.\n",
        "train_images = train_images.reshape((-1, 784))\n",
        "test_images = test_images.reshape((-1, 784))\n",
        "\n",
        "print(train_images.shape) # (60000, 784)\n",
        "print(test_images.shape)  # (10000, 784)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVGXHba9xq0s",
        "colab_type": "text"
      },
      "source": [
        "**Создание модели**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipIRGr2hxq0t",
        "colab_type": "text"
      },
      "source": [
        "После первичной подготовки данных дальше как правило следует создание модели нейронной сети, которая будет учиться на этих данных.\n",
        "\n",
        "Ниже типичный код учебной нейросети - "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C4PO9WLxq0u",
        "colab_type": "text"
      },
      "source": [
        "    # define the keras model\n",
        "    model = Sequential()\n",
        "    model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYHScGMqxq0v",
        "colab_type": "text"
      },
      "source": [
        "Давайте разберемся с теми командами, которые нам встетились в этом коде.\n",
        "\n",
        "Sequential - позволяет создать нейросети где слои имеют форму стека. Сигнал в них передается от одного слоя к другому. В противовес этой разновидности есть нейросети где сигнал может не сразу передаваться в следующий слой а попадать в цикл. Такие нейросети мы разберем в следующих уроках.\n",
        "\n",
        "Dense - позволяет каждому нейронну быть связанному с другим нейронном. В противовес этом может быть необходимость не делать так много связей. Неполносвязнные архитектуры мы также разберем на этом курсе, они основа компьютерного зрения.\n",
        "\n",
        "Цифры 12, 8, 1 обозначают количество нейронов в каждом конкретном слое\n",
        "\n",
        "Activation - позволяет определить формулу по которой будет активироваться нейрон."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OinpWAxwxq0v",
        "colab_type": "text"
      },
      "source": [
        "**Компиляция модели**\n",
        "\n",
        "На этапе компиляции модель с заданными параметрами ранее создается. Вот типичный учебный пример:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXbyi-nyxq0w",
        "colab_type": "text"
      },
      "source": [
        "    \n",
        "    # создание keras модели\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv8t-pCaxq0x",
        "colab_type": "text"
      },
      "source": [
        "Однако на этой стадии мы должны сделать еще некоторые настройки нейронной сети. Разберем команды из кода выше.\n",
        "\n",
        "loss - позволяет задать формулы по которой будет определяться степень ошибки нейронной сети.\n",
        "\n",
        "optimizer - позволяет задать алгоритм, который будет осуществлять изменения весов по всей нейронной сети (backpropagation)\n",
        "\n",
        "metrics - позволяет опредилить кретирии по которым будет оцениваться степень обученности нейросети.\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P_XbbvUxq0x",
        "colab_type": "text"
      },
      "source": [
        "**Передача данных для обучения нейросети**\n",
        "\n",
        "После того как нейросеть создана можно передавать ей данные для обучения. Ниже типичный пример кода для этого.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxhU42Kqxq0y",
        "colab_type": "text"
      },
      "source": [
        "    # передача обучающего датасета keras модели\n",
        "    model.fit(X, y, epochs=150, batch_size=10, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK5ydxRVxq0z",
        "colab_type": "text"
      },
      "source": [
        "Разберем команды из этого примера.\n",
        "X, y - содержат все обучающие данные\n",
        "epochs - определяет сколько раз через нейросеть должен пройти весь набор данных\n",
        "bath_size - определяет количество обучающих примеров передающихся нейросети на каждой итерации обучения.\n",
        "verbose - позволяет определять информацию, котору вы видете во время обучения нейронной сети."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H46UApozxq0z",
        "colab_type": "text"
      },
      "source": [
        "**Оценка обученности нейронной сети**\n",
        "\n",
        "Следующей стадией может быть проверка обученности нейронной сети. Команда Keras для этих целей - \n",
        "\n",
        "    results = model.evaluate(x_test, y_test, batch_size=128)\n",
        "    \n",
        "В данном случае мы просто указываем какую модель на каких данных мы хотим проверить"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIWoQg2Ixq00",
        "colab_type": "text"
      },
      "source": [
        "**Запуск нейронной сети для выполнения работы**\n",
        "\n",
        "На этой стадии мы можем попробовать запустить нейронную сеть на данных которые мы хотели бы чтобы она оценила. Осуществить распознования объекта на фотографии например.\n",
        "Вот код для этих целей - \n",
        "\n",
        "    predictions = model.predict(x_test[:3])\n",
        "    \n",
        "В качестве аргумента здесь указывается массив даныхх содержащих, например фотографию в виде массива чисел.    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pa0k3SNOxq00",
        "colab_type": "text"
      },
      "source": [
        "Мы с вами рассмотрели основные стадии процесса обучения нейросети и команды Keras, для этого. Безусловно здесь приведен далеко неполный перечень возможностей Keras. У Keras есть также возможность сохранять созданную нейросеть, запускать уже имеющиюся, различные средства для создания нейросетей разных архитектур и другое. С чем то из арсенала Keras мы с вами познакомимся по ходу курса, а с остальным вы можете познакомиться на сайте Keras в разделе документация."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_x34-0Ixq01",
        "colab_type": "text"
      },
      "source": [
        "## Простая нейросеть на Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-KgxR72xq02",
        "colab_type": "text"
      },
      "source": [
        "Давайте попрубуем сделать нейросеть на Keras использую полученные выше знания. Попробуем обучить нейросеть различать рукописные цифры."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7_oIwmtxq02",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "fac75601-f7e7-4095-e09d-0cc015397e91"
      },
      "source": [
        "# The full neural network code!\n",
        "###############################\n",
        "import numpy as np\n",
        "import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "train_images = mnist.train_images()\n",
        "train_labels = mnist.train_labels()\n",
        "test_images = mnist.test_images()\n",
        "test_labels = mnist.test_labels()\n",
        "\n",
        "# Normalize the images.\n",
        "train_images = (train_images / 255) - 0.5\n",
        "test_images = (test_images / 255) - 0.5\n",
        "\n",
        "# Flatten the images.\n",
        "train_images = train_images.reshape((-1, 784))\n",
        "test_images = test_images.reshape((-1, 784))\n",
        "\n",
        "# Build the model.\n",
        "model = Sequential([\n",
        "  Dense(64, activation='relu', input_shape=(784,)),\n",
        "  Dense(64, activation='relu'),\n",
        "  Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "# Compile the model.\n",
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# Train the model.\n",
        "model.fit(\n",
        "  train_images,\n",
        "  to_categorical(train_labels),\n",
        "  epochs=5,\n",
        "  batch_size=32,\n",
        ")\n",
        "\n",
        "# Evaluate the model.\n",
        "model.evaluate(\n",
        "  test_images,\n",
        "  to_categorical(test_labels)\n",
        ")\n",
        "\n",
        "# Save the model to disk.\n",
        "model.save_weights('model.h5')\n",
        "\n",
        "# Load the model from disk later using:\n",
        "# model.load_weights('model.h5')\n",
        "\n",
        "# Predict on the first 5 test images.\n",
        "predictions = model.predict(test_images[:5])\n",
        "\n",
        "# Print our model's predictions.\n",
        "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
        "\n",
        "# Check our predictions against the ground truths.\n",
        "print(test_labels[:5]) # [7, 2, 1, 0, 4]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 3s 56us/step - loss: 0.3613 - accuracy: 0.8909\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.1802 - accuracy: 0.9472\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.1428 - accuracy: 0.9553\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.1202 - accuracy: 0.9629\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.1050 - accuracy: 0.9671\n",
            "10000/10000 [==============================] - 0s 24us/step\n",
            "[7 2 1 0 4]\n",
            "[7 2 1 0 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SF0KXY0xq05",
        "colab_type": "text"
      },
      "source": [
        "## Практическое задание\n",
        "\n",
        "<ol>\n",
        "    <li>Попробуйте обучить нейронную сеть на Keras с другими параметрами. \n",
        "        Опишите в комментарии к уроку - какой результата вы добились от нейросети? Что помогло вам улучшить ее точность?</li>\n",
        "    <li>Поработайте с документацией Keras.</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L691zuvYxq06",
        "colab_type": "text"
      },
      "source": [
        "## Дополнительные материалы\n",
        "\n",
        "<ol>\n",
        "    <li>https://keras.io/</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y9FfdDUxq07",
        "colab_type": "text"
      },
      "source": [
        "## Используемая литература \n",
        "\n",
        "Для подготовки данного методического пособия были использованы следующие ресурсы:\n",
        "<ol>\n",
        "    <li>https://keras.io/</li>\n",
        "    <li>Шакла Н. — Машинное обучение и TensorFlow 2019</li>\n",
        "    <li>Википедия</li>\n",
        "    \n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FotGJFkYzz2n",
        "colab_type": "text"
      },
      "source": [
        "# Домашняя работа"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qKV6ni-0uJi",
        "colab_type": "text"
      },
      "source": [
        "## Увеличим число итераций (эпох)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfEj1pcvxq07",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "a1770513-c7dc-4c5c-ae82-755e9b4edc22"
      },
      "source": [
        "n_epochs = 10\n",
        "\n",
        "# Build the model.\n",
        "model = Sequential([\n",
        "  Dense(64, activation='relu', input_shape=(784,)),\n",
        "  Dense(64, activation='relu'),\n",
        "  Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "# Compile the model.\n",
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# Train the model.\n",
        "model.fit(\n",
        "  train_images,\n",
        "  to_categorical(train_labels),\n",
        "  epochs=n_epochs,\n",
        "  batch_size=32,\n",
        ")\n",
        "\n",
        "# Evaluate the model.\n",
        "model.evaluate(\n",
        "  test_images,\n",
        "  to_categorical(test_labels)\n",
        ")\n",
        "\n",
        "# Save the model to disk.\n",
        "model.save_weights('model.h5')\n",
        "\n",
        "# Load the model from disk later using:\n",
        "# model.load_weights('model.h5')\n",
        "\n",
        "# Predict on the first 5 test images.\n",
        "predictions = model.predict(test_images[:5])\n",
        "\n",
        "# Print our model's predictions.\n",
        "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
        "\n",
        "# Check our predictions against the ground truths.\n",
        "print(test_labels[:5]) # [7, 2, 1, 0, 4]\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.3436 - accuracy: 0.8959\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.1738 - accuracy: 0.9463\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.1351 - accuracy: 0.9583\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.1144 - accuracy: 0.9643\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0965 - accuracy: 0.9699\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0864 - accuracy: 0.9730\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0775 - accuracy: 0.9757\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0679 - accuracy: 0.9786\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0674 - accuracy: 0.9784\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0595 - accuracy: 0.9808\n",
            "10000/10000 [==============================] - 0s 25us/step\n",
            "[7 2 1 0 4]\n",
            "[7 2 1 0 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohny4M5135T3",
        "colab_type": "text"
      },
      "source": [
        "**Вывод:** accuracy повысилось по сравнению с исходным вариантом."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gk9StMs3bH-",
        "colab_type": "text"
      },
      "source": [
        "## Увеличим размер батчей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aY7tABDP2dvT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "91bbbe30-3bc2-4282-f8c6-3ab1b9b13e30"
      },
      "source": [
        "n_epochs = 5\n",
        "n_batch_size = 96\n",
        "\n",
        "# Build the model.\n",
        "model = Sequential([\n",
        "  Dense(64, activation='relu', input_shape=(784,)),\n",
        "  Dense(64, activation='relu'),\n",
        "  Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "# Compile the model.\n",
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# Train the model.\n",
        "model.fit(\n",
        "  train_images,\n",
        "  to_categorical(train_labels),\n",
        "  epochs=n_epochs,\n",
        "  batch_size=n_batch_size,\n",
        ")\n",
        "\n",
        "# Evaluate the model.\n",
        "model.evaluate(\n",
        "  test_images,\n",
        "  to_categorical(test_labels)\n",
        ")\n",
        "\n",
        "# Save the model to disk.\n",
        "model.save_weights('model.h5')\n",
        "\n",
        "# Load the model from disk later using:\n",
        "# model.load_weights('model.h5')\n",
        "\n",
        "# Predict on the first 5 test images.\n",
        "predictions = model.predict(test_images[:5])\n",
        "\n",
        "# Print our model's predictions.\n",
        "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
        "\n",
        "# Check our predictions against the ground truths.\n",
        "print(test_labels[:5]) # [7, 2, 1, 0, 4]\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 2s 27us/step - loss: 0.4477 - accuracy: 0.8705\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 0.2275 - accuracy: 0.9329\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 0.1749 - accuracy: 0.9472\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 0.1443 - accuracy: 0.9567\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 2s 26us/step - loss: 0.1233 - accuracy: 0.9629\n",
            "10000/10000 [==============================] - 0s 24us/step\n",
            "[7 2 1 0 4]\n",
            "[7 2 1 0 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIm7vTF-4W81",
        "colab_type": "text"
      },
      "source": [
        "**Вывод:** по сравнению с исходным вариантом улучшений нет."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exMR6UQp5duO",
        "colab_type": "text"
      },
      "source": [
        "## Увеличим количество нейронов в каждом слое"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh48f-vp468d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "7399074f-b31f-442c-b99f-5827054a9335"
      },
      "source": [
        "n_epochs = 5\n",
        "n_batch_size = 32\n",
        "\n",
        "# Build the model.\n",
        "model = Sequential([\n",
        "  Dense(64*2, activation='relu', input_shape=(784,)),\n",
        "  Dense(64*2, activation='relu'),\n",
        "  Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "# Compile the model.\n",
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# Train the model.\n",
        "model.fit(\n",
        "  train_images,\n",
        "  to_categorical(train_labels),\n",
        "  epochs=n_epochs,\n",
        "  batch_size=n_batch_size,\n",
        ")\n",
        "\n",
        "# Evaluate the model.\n",
        "model.evaluate(\n",
        "  test_images,\n",
        "  to_categorical(test_labels)\n",
        ")\n",
        "\n",
        "# Save the model to disk.\n",
        "model.save_weights('model.h5')\n",
        "\n",
        "# Load the model from disk later using:\n",
        "# model.load_weights('model.h5')\n",
        "\n",
        "# Predict on the first 5 test images.\n",
        "predictions = model.predict(test_images[:5])\n",
        "\n",
        "# Print our model's predictions.\n",
        "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
        "\n",
        "# Check our predictions against the ground truths.\n",
        "print(test_labels[:5]) # [7, 2, 1, 0, 4]\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 5s 89us/step - loss: 0.3077 - accuracy: 0.9062\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.1499 - accuracy: 0.9536\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.1138 - accuracy: 0.9642\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 5s 87us/step - loss: 0.0939 - accuracy: 0.9707\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0822 - accuracy: 0.9736\n",
            "10000/10000 [==============================] - 0s 28us/step\n",
            "[7 2 1 0 4]\n",
            "[7 2 1 0 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWz7jf1552dA",
        "colab_type": "text"
      },
      "source": [
        "**Выводы:**\n",
        "1. Время работы увеличилось;\n",
        "2. Результат работы по сравнению с исходным улучшился несущественно."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHFkttfU6aea",
        "colab_type": "text"
      },
      "source": [
        "## Изменим количество слоев"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIpvjvpZ6YZC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "98b9a718-decd-45f0-8591-5a5bf7df8f91"
      },
      "source": [
        "n_epochs = 5\n",
        "n_batch_size = 32\n",
        "\n",
        "# Build the model.\n",
        "model = Sequential([\n",
        "  Dense(64, activation='relu', input_shape=(784,)),\n",
        "  Dense(64, activation='relu'),\n",
        "  Dense(32, activation='relu'),\n",
        "  Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "# Compile the model.\n",
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss='categorical_crossentropy',\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# Train the model.\n",
        "model.fit(\n",
        "  train_images,\n",
        "  to_categorical(train_labels),\n",
        "  epochs=n_epochs,\n",
        "  batch_size=n_batch_size,\n",
        ")\n",
        "\n",
        "# Evaluate the model.\n",
        "model.evaluate(\n",
        "  test_images,\n",
        "  to_categorical(test_labels)\n",
        ")\n",
        "\n",
        "# Save the model to disk.\n",
        "model.save_weights('model.h5')\n",
        "\n",
        "# Load the model from disk later using:\n",
        "# model.load_weights('model.h5')\n",
        "\n",
        "# Predict on the first 5 test images.\n",
        "predictions = model.predict(test_images[:5])\n",
        "\n",
        "# Print our model's predictions.\n",
        "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
        "\n",
        "# Check our predictions against the ground truths.\n",
        "print(test_labels[:5]) # [7, 2, 1, 0, 4]\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 3s 57us/step - loss: 0.3692 - accuracy: 0.8872\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.1865 - accuracy: 0.9426\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.1481 - accuracy: 0.9534\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.1269 - accuracy: 0.9606\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.1091 - accuracy: 0.9661\n",
            "10000/10000 [==============================] - 0s 25us/step\n",
            "[7 2 1 0 4]\n",
            "[7 2 1 0 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de4YDfLE6rTA",
        "colab_type": "text"
      },
      "source": [
        "**Вывод:** результат не лучше, чем в исходном варианте."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRq6etx068dX",
        "colab_type": "text"
      },
      "source": [
        "## Выводы по работе:\n",
        "1. Удалось повысить точность с 0.9671 до 0.9808 за счет увеличения числа итераций;\n",
        "2. Можно было бы еще поэкспериментировать с параметром activation. И вообще, много с чем можно экспериментировать;\n",
        "3. Можно было бы комбинировать изменения различных параметров для подбора их оптимального набора."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H1XNhgu7xAy",
        "colab_type": "text"
      },
      "source": [
        "## 2. Поработайте с документацией Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppxzcJAI7zgL",
        "colab_type": "text"
      },
      "source": [
        "На мой взгляд интересной выглядит возможность создания своей loss-функции.\n",
        "\n",
        "Например:\n",
        "\n",
        "```\n",
        "def my_loss_fn(y_true, y_pred):\n",
        "    squared_difference = tf.square(y_true - y_pred)\n",
        "    return tf.reduce_mean(squared_difference, axis=-1)  # Note the axis=-1`\n",
        "\n",
        "model.compile(optimizer='adam', loss=my_loss_fn)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbsnjpxJ66Vn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}